{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ra4uAoXwKDVI"
   },
   "source": [
    "# **Classification using Unified Similarity Measure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T05:57:44.021109Z",
     "start_time": "2021-04-21T05:57:38.882327Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83
    },
    "colab_type": "code",
    "id": "Uo_NlNDhSKXH",
    "outputId": "67e4030c-a6f1-42a4-c974-b86b687b6adc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdul\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abdul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abdul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import numpy as np\n",
    "from gensim import corpora,models\n",
    "import time\n",
    "import pickle\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "from gensim.models import Word2Vec,FastText\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import distance\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T05:57:44.052998Z",
     "start_time": "2021-04-21T05:57:44.025071Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "colab_type": "code",
    "id": "4n62yJhof_Fv",
    "outputId": "fd5df092-0edd-436f-938f-c9066c9036bc"
   },
   "outputs": [],
   "source": [
    "# !pip install glove_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T05:57:44.116910Z",
     "start_time": "2021-04-21T05:57:44.056986Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "OE2FYbCkgGxJ"
   },
   "outputs": [],
   "source": [
    "from glove import Glove, Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T05:57:44.132867Z",
     "start_time": "2021-04-21T05:57:44.120899Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "id": "J6dMMgPXSO4s",
    "outputId": "f1223bbf-e4b7-4635-b2e9-84ae767d0878"
   },
   "outputs": [],
   "source": [
    "#Mounting google drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T05:57:44.730493Z",
     "start_time": "2021-04-21T05:57:44.135862Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "UGn76yTmSRLd",
    "outputId": "49ea23d5-8930-44b8-c344-e170a417e113"
   },
   "outputs": [],
   "source": [
    "# open a file, where you stored the pickled data\n",
    "f = open('CSV/bow_corpus.pickle', 'rb')\n",
    "bow_corpus=pickle.load(f)\n",
    "\n",
    "file = open('CSV/dictionary.pickle', 'rb')\n",
    "dictionary=pickle.load(file)\n",
    "\n",
    "# later on, load trained model from file\n",
    "lda_model =  models.LdaModel.load('CSV/lda_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T05:57:44.745452Z",
     "start_time": "2021-04-21T05:57:44.734482Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "mqJMzJoFSqX3"
   },
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    return WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 5:\n",
    "            result.append(lemmatize(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T05:58:03.969464Z",
     "start_time": "2021-04-21T05:57:44.750439Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "WEt1TXMYScSZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# importing all the clusters created using LDA based topic modeling\n",
    "for c in range(10):\n",
    "  exec('topic_{} = pd.read_csv(\"CSV/topic_{}.csv\")'.format(c,c))\n",
    "  exec(\"topic_{}= topic_{}.drop(columns=['Unnamed: 0'])\".format(c,c))\n",
    "  exec(\"topic_{}['Description'] = topic_{}['Description'].map(preprocess)\".format(c,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T05:58:04.122979Z",
     "start_time": "2021-04-21T05:58:03.974376Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "p8ar7N6mfBSN"
   },
   "outputs": [],
   "source": [
    "#import the duplicate reports for testing purpose\n",
    "test = pd.read_csv('CSV/duplicate_reports.csv')\n",
    "test = test.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T05:58:07.690977Z",
     "start_time": "2021-04-21T05:58:04.125972Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "V1NfsKaphgGs"
   },
   "outputs": [],
   "source": [
    "test['Description']= test['Description'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T05:58:35.468778Z",
     "start_time": "2021-04-21T05:58:07.693963Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "3IsHybD6SlgC",
    "outputId": "25e968b6-c076-4ee0-c104-48f218bb1310"
   },
   "outputs": [],
   "source": [
    "for mod in range(10):\n",
    "  #import all the trained Word2Vec models\n",
    "  exec('w2vmodel{} = Word2Vec.load(\"CSV/word2vec{}.model\")'.format(mod, mod))\n",
    "\n",
    "  #import all the trained FastText models\n",
    "  exec('ftmodel{} = FastText.load(\"CSV/ftmodel{}.model\")'.format(mod, mod))\n",
    "\n",
    "  #import all the trained GloVe models\n",
    "  exec('glove{} = Glove.load(\"CSV/glove{}.model\")'.format(mod, mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T05:58:35.484737Z",
     "start_time": "2021-04-21T05:58:35.471773Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ZleuynPDU2GI"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wYKIDs4VKhFg"
   },
   "source": [
    "### **Selection of Top-n clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T05:58:35.515654Z",
     "start_time": "2021-04-21T05:58:35.491719Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "S6FFJvIdVVBZ"
   },
   "outputs": [],
   "source": [
    "#This will return the index of cluster in which the master report of duplicate report may reside\n",
    "def sim_with_clusters_lda_topn(DR, n):\n",
    "    vec_bow = dictionary.doc2bow(DR)\n",
    "    x= lda_model[vec_bow]\n",
    "    topic = np.asarray(x)\n",
    "    # max_sim = int(topic[np.argmax(topic[:,1]),0]) \n",
    "    # max_sim\n",
    "    sim=[]\n",
    "    x= topic[np.argsort(topic[:,1])[-n:][::-1],0]\n",
    "    for i in range(len(x)):\n",
    "        sim.append(int(x[i]))\n",
    "    # return max_sim\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T05:58:35.546571Z",
     "start_time": "2021-04-21T05:58:35.520640Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "LsPY5Sicg0ZC"
   },
   "outputs": [],
   "source": [
    "# Convert multiple word embeddings into a single document vector by averaging the word embeddings by GloVe model\n",
    "\n",
    "def average_word_vectors_glove(words, model, vocabulary, num_features):  \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.  \n",
    "\n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model.word_vectors[model.dictionary[word]])\n",
    "\n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "    \n",
    "\n",
    "\n",
    "def averaged_word_vectorizer_glove(corpus, model, num_features):\n",
    "    vocabulary = set(model.dictionary)\n",
    "    if(any(isinstance(i, list) for i in corpus)):\n",
    "      features = [average_word_vectors_glove(tokenized_sentence, model, vocabulary, num_features)\n",
    "                      for tokenized_sentence in corpus]\n",
    "      return np.array(features)\n",
    "    else:\n",
    "      features = average_word_vectors_glove(corpus, model, vocabulary, num_features)\n",
    "      return np.array(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T05:58:35.578485Z",
     "start_time": "2021-04-21T05:58:35.552563Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "BGo2DWb-TvvZ"
   },
   "outputs": [],
   "source": [
    "# Convert multiple word embeddings into a single document vector by averaging the word embeddings by FastText or Word2Vec model\n",
    "\n",
    "def average_word_vectors_w2v(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "    \n",
    "def averaged_word_vectorizer_w2v(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    if(any(isinstance(i, list) for i in corpus)):\n",
    "      features = [average_word_vectors_w2v(tokenized_sentence, model, vocabulary, num_features)\n",
    "                      for tokenized_sentence in corpus]\n",
    "      return np.array(features)\n",
    "    else:\n",
    "      features = average_word_vectors_w2v(corpus, model, vocabulary, num_features)\n",
    "      return np.array(features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cievu27fKtCH"
   },
   "source": [
    "### **Unified Similarity Measure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T05:58:35.594445Z",
     "start_time": "2021-04-21T05:58:35.581479Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "mSrDdldatps-"
   },
   "outputs": [],
   "source": [
    "# Similarity between two feature vectors using the average of cosine similarity and euclidean similarity\n",
    "def sim(vec1, vec2): \n",
    "  sim1 = 1/(1+distance.euclidean(vec1, vec2))\n",
    "  sim2 = cosine_similarity(vec1, vec2)\n",
    "  sim=(sim1+sim2)/2 \n",
    "  return sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FUB6hIlrKykY"
   },
   "source": [
    "### **Multimodality Fusions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T05:58:35.624362Z",
     "start_time": "2021-04-21T05:58:35.599430Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "z4A0FX-ry0B0"
   },
   "outputs": [],
   "source": [
    "# Different kinds of fusion of two master report feature vectors and two duplicate report feature vectors\n",
    "def fusion(vec1, vec2, vec3, vec4, fusion_no):\n",
    "\n",
    "  # fusion_no = 1 : concatenation of the vectors\n",
    "  if (fusion_no == '1'):\n",
    "    master = np.concatenate((vec1, vec2), axis=1)\n",
    "    vec_duplicate = np.concatenate((vec3, vec4), axis=0)\n",
    "    vec_duplicate=[vec_duplicate]\n",
    "    return vec_duplicate, master\n",
    "\n",
    "  #fusion_no = 2 : average of the vectors\n",
    "  elif (fusion_no == '2'):\n",
    "    vec3 = [vec3]\n",
    "    vec4 = [vec4]\n",
    "    avg1 = (np.add(vec1, vec2))/2\n",
    "    avg2 = (np.add(vec3, vec4))/2\n",
    "    return avg2, avg1\n",
    "\n",
    "  #fusion_no = 3 : Dimensionality reduction using PCA on concatenation of the vectors\n",
    "  elif (fusion_no == '3'):\n",
    "    master = np.concatenate((vec1, vec2), axis=1)\n",
    "    pca = PCA(n_components=100)\n",
    "    avg_fit = pca.fit(master)\n",
    "    master = pca.transform(master)\n",
    "    vec_duplicate = np.concatenate((vec3, vec4), axis=0)\n",
    "    vec_duplicate=[vec_duplicate]\n",
    "    vec_duplicate = pca.transform(vec_duplicate)\n",
    "    return vec_duplicate, master\n",
    "\n",
    "  #fusion_no = 3 : Dimensionality reduction using PCA on average of the vectors\n",
    "  elif (fusion_no == '4'):\n",
    "    vec3 = [vec3]\n",
    "    vec4 = [vec4]\n",
    "    avg1 = (np.add(vec1, vec2))/2\n",
    "    pca = PCA(n_components=100)\n",
    "    avg_fit = pca.fit(avg1)\n",
    "    master = pca.transform(avg1)\n",
    "    avg2 = (np.add(vec3, vec4))/2\n",
    "    vec_duplicate = pca.transform(avg2)\n",
    "    return vec_duplicate, master\n",
    "\n",
    "  else:\n",
    "    raise ValueError('Invalid value for fusion')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SQ1TY4TrK9_0"
   },
   "source": [
    "### **Creation of Feature Vectors using Multimodality and Single modality Feature Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T05:58:35.640321Z",
     "start_time": "2021-04-21T05:58:35.627356Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Nii-n-0GsaKi"
   },
   "outputs": [],
   "source": [
    "# creation of feature vectors by multimodality feature extraction\n",
    "def feature_vectors_multi_modality(DR, corpus, model1, model2, fusion_no):\n",
    "  master_ft1 = averaged_word_vectorizer_w2v(corpus=sent, model=model1, num_features=100)\n",
    "  master_glove2 = averaged_word_vectorizer_glove(corpus=sent, model=model2, num_features=100)\n",
    "\n",
    "  vec_duplicate1 = averaged_word_vectorizer_w2v(corpus=DR, model=model1, num_features=100)\n",
    "  vec_duplicate2 = averaged_word_vectorizer_glove(corpus=DR, model=model2, num_features=100)\n",
    "\n",
    "  #for fusion 1 and fusion 3 :\n",
    "  # vec_duplicate, master= fusion_3(master_ft1, master_glove2, vec_duplicate1, vec_duplicate2)\n",
    "\n",
    "  #for this for fusion 2 and 4:\n",
    "  vec_duplicate , master= fusion(master_ft1, master_glove2, vec_duplicate1, vec_duplicate2, fusion_no)\n",
    "\n",
    "  return vec_duplicate,master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T05:58:35.656276Z",
     "start_time": "2021-04-21T05:58:35.643313Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "u42AbKQ6GOiW"
   },
   "outputs": [],
   "source": [
    "# creation of feature vectors by singlemodality feature extraction\n",
    "def feature_vectors_single_modality(DR, corpus, model1):\n",
    "  master = averaged_word_vectorizer_w2v(corpus=sent, model=model1, num_features=100)\n",
    "\n",
    "  vec_duplicate = averaged_word_vectorizer_w2v(corpus=DR, model=model1, num_features=100)\n",
    "\n",
    "  vec_duplicate = [vec_duplicate]\n",
    "\n",
    "  return vec_duplicate, master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nIFp0iS1LIou"
   },
   "source": [
    "### **Top-N Recommendations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T05:58:35.686200Z",
     "start_time": "2021-04-21T05:58:35.659269Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "SMbKNltgU7H8"
   },
   "outputs": [],
   "source": [
    "# Returns Top-N master reports\n",
    "\n",
    "def compare_topn(model1, model2, cluster, sent, DR, topn, modal, fusion_no):\n",
    "  similarity=[]\n",
    "\n",
    "  if (modal == 'multi'):\n",
    "  #create feature vectors for duplicate and master reports using multimodality\n",
    "    vec_duplicate, master= feature_vectors_multi_modality(DR, sent, model1, model2, fusion_no)\n",
    "\n",
    "  # #create feature vectors for duplicate and master reports using single modality\n",
    "  elif (modal == 'single'):\n",
    "    vec_duplicate, master= feature_vectors_single_modality(DR, sent, model1)\n",
    "\n",
    "  else:\n",
    "    raise ValueError('Invalid Modality entered')\n",
    "\n",
    "  for doc in range(len(master)):\n",
    "    vec_master = master[doc]\n",
    "    vec_master= [vec_master]\n",
    "    unified_sim = sim(vec_duplicate, vec_master)\n",
    "\n",
    "    similarity.append(unified_sim)\n",
    "  similarity = np.asarray(similarity)\n",
    "  similarity= np.concatenate(similarity, axis=0 )\n",
    "  similarity= np.concatenate(similarity, axis=0 )\n",
    "  max_similar_reports=similarity.argsort()[-topn:][::-1]\n",
    "  # # # for d,f in enumerate(max_similar_reports):\n",
    "  # # #     similar_reports= similar_reports.append(cluster.loc[[f]])\n",
    "  return(max_similar_reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2iW_dNSPLOPz"
   },
   "source": [
    "### **Evalation of the Approach using Recall Rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T05:58:36.719367Z",
     "start_time": "2021-04-21T05:58:35.692182Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "HFWTB2VlVF_E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'FastText' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-37142688d872>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mmodal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'multi'\u001b[0m                                    \u001b[1;31m#Whether you want to use single feature extraction model or multi model ( for single, it'll consider just model1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m      \u001b[1;31m#This will return the Top-N predicted master reports\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mmax_sim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompare_topn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfusion_no\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mt2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-a23fc274c6e5>\u001b[0m in \u001b[0;36mcompare_topn\u001b[1;34m(model1, model2, cluster, sent, DR, topn, modal, fusion_no)\u001b[0m\n\u001b[0;32m      6\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodal\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'multi'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m   \u001b[1;31m#create feature vectors for duplicate and master reports using multimodality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mvec_duplicate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaster\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mfeature_vectors_multi_modality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfusion_no\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m   \u001b[1;31m# #create feature vectors for duplicate and master reports using single modality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-4456701081fe>\u001b[0m in \u001b[0;36mfeature_vectors_multi_modality\u001b[1;34m(DR, corpus, model1, model2, fusion_no)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# creation of feature vectors by multimodality feature extraction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfeature_vectors_multi_modality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfusion_no\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m   \u001b[0mmaster_ft1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maveraged_word_vectorizer_w2v\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m   \u001b[0mmaster_glove2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maveraged_word_vectorizer_glove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-7f29205108e9>\u001b[0m in \u001b[0;36maveraged_word_vectorizer_w2v\u001b[1;34m(corpus, model, num_features)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_to_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m       features = [average_word_vectors_w2v(tokenized_sentence, model, vocabulary, num_features)\n\u001b[0m\u001b[0;32m     22\u001b[0m                       for tokenized_sentence in corpus]\n\u001b[0;32m     23\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-7f29205108e9>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_to_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m       features = [average_word_vectors_w2v(tokenized_sentence, model, vocabulary, num_features)\n\u001b[0m\u001b[0;32m     22\u001b[0m                       for tokenized_sentence in corpus]\n\u001b[0;32m     23\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-7f29205108e9>\u001b[0m in \u001b[0;36maverage_word_vectors_w2v\u001b[1;34m(words, model, vocabulary, num_features)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mnwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnwords\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mfeature_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'FastText' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Recall Rate for Top-2.5K reports (Because Top-N where N = n * topn (2.5K = 3*833)) \n",
    "vec_acc=[]\n",
    "t1 = time.time()\n",
    "no_of_test_samples= int(200)\n",
    "for i in range(no_of_test_samples):\n",
    "  sample = test.Description[i] #The test sample (duplicate report)\n",
    "  n = 3\n",
    "  max_cluster =sim_with_clusters_lda_topn(sample, n)\n",
    "  v=[]\n",
    "  print(i)\n",
    "  for max in max_cluster:\n",
    "    exec('cluster = topic_{}'.format(max))              #The predicted cluster\n",
    "    exec('model1 = ftmodel{}'.format(max))              #The trained FastText model for the predicted cluster   (can be changed to other model as well viz. glove or word2vec)\n",
    "    exec('model2 = glove{}'.format(max))                #The trained Word2Vec model for the predicted cluster   (Doesn't count if using single modality)\n",
    "    exec('sent = topic_{}.Description'.format(max))     #The vocabulary for the predicted cluster\n",
    "    topn = 833                                          #The number of predicted master report for single predicted cluster\n",
    "    fusion_no = '4'   #Doesn't count if single modality #The selection of fusion used to fuse the word embeddings of two different models  (4 gives the best results)\n",
    "    modal = 'multi'                                    #Whether you want to use single feature extraction model or multi model ( for single, it'll consider just model1)\n",
    "     #This will return the Top-N predicted master reports\n",
    "    max_sim = compare_topn(model1, model2, cluster, sent, sample, topn, modal, fusion_no)\n",
    "    t2 = time.time()\n",
    "\n",
    "    #Comparing the predicted value to the ground truth\n",
    "    for num in max_sim:\n",
    "      if (cluster.Issue_id[num] == test.Duplicated_issue[i]):\n",
    "          v.append(\"1\")\n",
    "      else:\n",
    "          v.append(\"0\")\n",
    "  \n",
    "  if(all(x==v[0] for x in v)):\n",
    "    vec_acc.append(\"0\")\n",
    "  else:\n",
    "    vec_acc.append(\"1\")\n",
    "\n",
    "#Evaluating the performance by Recall Rate\n",
    "sum = 0\n",
    "for i,num in enumerate(vec_acc):\n",
    "    sum = sum + int(num)\n",
    "recall_rate = (sum/len(vec_acc))*100\n",
    "print(\"Recall Rate : {} %\".format(recall_rate))\n",
    "print(\"Time : \", (t2-t1)/60, \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a0WnRnwfIbOY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
